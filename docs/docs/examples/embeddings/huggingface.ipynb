{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNxy-Erkw8si"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJX8-Hu-w8sk"
      },
      "source": [
        "# Local Embeddings with HuggingFace\n",
        "\n",
        "LlamaIndex has support for HuggingFace embedding models, including BGE, Instructor, and more.\n",
        "\n",
        "Furthermore, we provide utilities to create and use ONNX models using the [Optimum library](https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-optimumonnxruntime) from HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvCod5Vw8sk"
      },
      "source": [
        "## HuggingFaceEmbedding\n",
        "\n",
        "The base `HuggingFaceEmbedding` class is a generic wrapper around any HuggingFace model for embeddings. All [embedding models](https://huggingface.co/models?library=sentence-transformers) on Hugging Face should work. You can refer to the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more recommendations.\n",
        "\n",
        "This class depends on the sentence-transformers package, which you can install with `pip install sentence-transformers`.\n",
        "\n",
        "NOTE: if you were previously using a `HuggingFaceEmbeddings` from LangChain, this should give equivalent results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUpq6UkIw8sk"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SlLS7HH2w8sl",
        "outputId": "9b72d78a-23ea-4040-a405-16c8f8a65d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.26.2)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.0 (from llama-index-embeddings-huggingface)\n",
            "  Downloading llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.10)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.0.36)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (11.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.46.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.20.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.2)\n",
            "Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, dirtyjson, tenacity, mypy-extensions, marshmallow, typing-inspect, tiktoken, dataclasses-json, llama-index-core, llama-index-embeddings-huggingface\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-index-core-0.11.23 llama-index-embeddings-huggingface-0.3.1 marshmallow-3.23.1 mypy-extensions-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n",
            "Collecting llama-index-embeddings-instructor\n",
            "  Downloading llama_index_embeddings_instructor-0.2.1-py3-none-any.whl.metadata (759 bytes)\n",
            "Collecting instructorembedding<2.0.0,>=1.0.1 (from llama-index-embeddings-instructor)\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-instructor) (0.11.23)\n",
            "Collecting sentence-transformers<3.0.0,>=2.2.2 (from llama-index-embeddings-instructor)\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-instructor) (2.5.0+cu121)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (11.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.16.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (4.46.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (3.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (4.0.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.1.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (0.20.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.23.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (3.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (3.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-instructor) (1.2.2)\n",
            "Downloading llama_index_embeddings_instructor-0.2.1-py3-none-any.whl (3.6 kB)\n",
            "Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: instructorembedding, sentence-transformers, llama-index-embeddings-instructor\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.2.1\n",
            "    Uninstalling sentence-transformers-3.2.1:\n",
            "      Successfully uninstalled sentence-transformers-3.2.1\n",
            "Successfully installed instructorembedding-1.0.1 llama-index-embeddings-instructor-0.2.1 sentence-transformers-2.7.0\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-embeddings-instructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a5Xi9V8xw8sm",
        "outputId": "d42c3e61-0168-4ce4-c3bb-7d46e9011d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.23-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.23 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.11.23)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.54.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (11.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.5-py3-none-any.whl.metadata (763 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
            "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.2.0)\n",
            "Downloading llama_index-0.11.23-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.3.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_cloud-0.1.5-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.14-py3-none-any.whl (13 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: striprtf, pypdf, llama-cloud, llama-index-legacy, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed llama-cloud-0.1.5 llama-index-0.11.23 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.2 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.3.0 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.14 pypdf-5.1.0 striprtf-0.0.26\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/agentless_s\n",
        "!mkdir testbed"
      ],
      "metadata": {
        "id": "mhbHKKBdxkxU",
        "outputId": "9930fee5-f260-4e97-8594-c8a3b5b78f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/agentless_s'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/django/django.git testbed/django\n",
        "!git clone https://github.com/sympy/sympy.git testbed/sympy\n",
        "!git clone https://github.com/matplotlib/matplotlib.git testbed/matplotlib\n",
        "!git clone https://github.com/scikit-learn/scikit-learn.git testbed/scikit-learn\n",
        "!git clone https://github.com/pytest-dev/pytest.git testbed/pytest\n",
        "!git clone https://github.com/sphinx-doc/sphinx.git testbed/sphinx\n",
        "!git clone https://github.com/psf/requests.git testbed/requests\n",
        "!git clone https://github.com/pylint-dev/pylint.git testbed/pylint\n",
        "!git clone https://github.com/astropy/astropy.git testbed/astropy\n",
        "!git clone https://github.com/pydata/xarray.git testbed/xarray\n",
        "!git clone https://github.com/mwaskom/seaborn.git testbed/seaborn\n",
        "!git clone https://github.com/pallets/flask.git testbed/flask"
      ],
      "metadata": {
        "id": "Bc6CAIjI_OKg",
        "outputId": "d74ea7eb-5687-47e9-d701-f37fea3926ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'testbed/django'...\n",
            "remote: Enumerating objects: 534173, done.\u001b[K\n",
            "remote: Counting objects: 100% (474/474), done.\u001b[K\n",
            "remote: Compressing objects: 100% (283/283), done.\u001b[K\n",
            "remote: Total 534173 (delta 291), reused 291 (delta 190), pack-reused 533699 (from 1)\u001b[K\n",
            "Receiving objects: 100% (534173/534173), 258.23 MiB | 31.65 MiB/s, done.\n",
            "Resolving deltas: 100% (383307/383307), done.\n",
            "Cloning into 'testbed/sympy'...\n",
            "remote: Enumerating objects: 410069, done.\u001b[K\n",
            "remote: Counting objects: 100% (2065/2065), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1009/1009), done.\u001b[K\n",
            "remote: Total 410069 (delta 1348), reused 1581 (delta 1050), pack-reused 408004 (from 1)\u001b[K\n",
            "Receiving objects: 100% (410069/410069), 183.53 MiB | 34.41 MiB/s, done.\n",
            "Resolving deltas: 100% (326912/326912), done.\n",
            "Cloning into 'testbed/matplotlib'...\n",
            "remote: Enumerating objects: 332527, done.\u001b[K\n",
            "remote: Counting objects: 100% (2009/2009), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1464/1464), done.\u001b[K\n",
            "remote: Total 332527 (delta 1122), reused 1205 (delta 542), pack-reused 330518 (from 1)\u001b[K\n",
            "Receiving objects: 100% (332527/332527), 446.28 MiB | 41.88 MiB/s, done.\n",
            "Resolving deltas: 100% (230612/230612), done.\n",
            "Cloning into 'testbed/scikit-learn'...\n",
            "remote: Enumerating objects: 253197, done.\u001b[K\n",
            "remote: Counting objects: 100% (748/748), done.\u001b[K\n",
            "remote: Compressing objects: 100% (575/575), done.\u001b[K\n",
            "remote: Total 253197 (delta 410), reused 366 (delta 173), pack-reused 252449 (from 1)\u001b[K\n",
            "Receiving objects: 100% (253197/253197), 160.92 MiB | 34.29 MiB/s, done.\n",
            "Resolving deltas: 100% (196608/196608), done.\n",
            "Cloning into 'testbed/pytest'...\n",
            "remote: Enumerating objects: 107548, done.\u001b[K\n",
            "remote: Counting objects: 100% (655/655), done.\u001b[K\n",
            "remote: Compressing objects: 100% (383/383), done.\u001b[K\n",
            "remote: Total 107548 (delta 376), reused 455 (delta 240), pack-reused 106893 (from 1)\u001b[K\n",
            "Receiving objects: 100% (107548/107548), 34.06 MiB | 30.97 MiB/s, done.\n",
            "Resolving deltas: 100% (75639/75639), done.\n",
            "Cloning into 'testbed/sphinx'...\n",
            "remote: Enumerating objects: 171032, done.\u001b[K\n",
            "remote: Counting objects: 100% (2551/2551), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1312/1312), done.\u001b[K\n",
            "remote: Total 171032 (delta 1476), reused 2034 (delta 1090), pack-reused 168481 (from 1)\u001b[K\n",
            "Receiving objects: 100% (171032/171032), 95.38 MiB | 29.32 MiB/s, done.\n",
            "Resolving deltas: 100% (125938/125938), done.\n",
            "Cloning into 'testbed/requests'...\n",
            "remote: Enumerating objects: 25993, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 25993 (delta 35), reused 40 (delta 21), pack-reused 25924 (from 1)\u001b[K\n",
            "Receiving objects: 100% (25993/25993), 12.73 MiB | 30.66 MiB/s, done.\n",
            "Resolving deltas: 100% (17063/17063), done.\n",
            "Cloning into 'testbed/pylint'...\n",
            "remote: Enumerating objects: 88180, done.\u001b[K\n",
            "remote: Counting objects: 100% (1249/1249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (811/811), done.\u001b[K\n",
            "remote: Total 88180 (delta 719), reused 829 (delta 422), pack-reused 86931 (from 1)\u001b[K\n",
            "Receiving objects: 100% (88180/88180), 41.00 MiB | 32.05 MiB/s, done.\n",
            "Resolving deltas: 100% (58824/58824), done.\n",
            "Cloning into 'testbed/astropy'...\n",
            "remote: Enumerating objects: 281661, done.\u001b[K\n",
            "remote: Counting objects: 100% (3131/3131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1369/1369), done.\u001b[K\n",
            "remote: Total 281661 (delta 2162), reused 2526 (delta 1761), pack-reused 278530 (from 1)\u001b[K\n",
            "Receiving objects: 100% (281661/281661), 162.94 MiB | 32.09 MiB/s, done.\n",
            "Resolving deltas: 100% (217868/217868), done.\n",
            "Cloning into 'testbed/xarray'...\n",
            "remote: Enumerating objects: 45435, done.\u001b[K\n",
            "remote: Counting objects: 100% (1219/1219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (698/698), done.\u001b[K\n",
            "remote: Total 45435 (delta 860), reused 756 (delta 521), pack-reused 44216 (from 1)\u001b[K\n",
            "Receiving objects: 100% (45435/45435), 47.85 MiB | 36.48 MiB/s, done.\n",
            "Resolving deltas: 100% (36195/36195), done.\n",
            "Cloning into 'testbed/seaborn'...\n",
            "remote: Enumerating objects: 19125, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 19125 (delta 2), reused 10 (delta 2), pack-reused 19112 (from 1)\u001b[K\n",
            "Receiving objects: 100% (19125/19125), 53.04 MiB | 41.37 MiB/s, done.\n",
            "Resolving deltas: 100% (14058/14058), done.\n",
            "Cloning into 'testbed/flask'...\n",
            "remote: Enumerating objects: 25263, done.\u001b[K\n",
            "remote: Counting objects: 100% (349/349), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 25263 (delta 198), reused 258 (delta 142), pack-reused 24914 (from 1)\u001b[K\n",
            "Receiving objects: 100% (25263/25263), 10.44 MiB | 26.13 MiB/s, done.\n",
            "Resolving deltas: 100% (16894/16894), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Functions\n",
        "import subprocess\n",
        "def checkout_commit(repo_path, commit_id):\n",
        "    \"\"\"Checkout the specified commit in the given local git repository.\n",
        "    First discards any untracked changes in the repository.\n",
        "    :param repo_path: Path to the local git repository\n",
        "    :param commit_id: Commit ID to checkout\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clean untracked files and directories\n",
        "        print(f\"Cleaning untracked files in repository at {repo_path}...\")\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"clean\", \"-fd\"], check=True)\n",
        "\n",
        "        # Discard changes in tracked files\n",
        "        print(\"Discarding changes in tracked files...\")\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"reset\", \"--hard\"], check=True)\n",
        "\n",
        "        # Change directory to the provided repository path and checkout the specified commit\n",
        "        print(f\"Checking out commit {commit_id}...\")\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"checkout\", commit_id], check=True)\n",
        "        print(\"Commit checked out successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred while running git command: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "def get_all_file_paths(folder_path, base_dir):\n",
        "    \"\"\"\n",
        "    Returns a list of absolute file paths for all files in the given folder and its subfolders.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path to the folder to search\n",
        "\n",
        "    Returns:\n",
        "        list: A list of absolute file paths\n",
        "    \"\"\"\n",
        "    file_paths = []\n",
        "\n",
        "    # Convert to absolute path if relative path is given\n",
        "    folder_path = os.path.abspath(folder_path)\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise ValueError(f\"The folder '{folder_path}' does not exist\")\n",
        "\n",
        "    # Walk through the directory tree\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        # Add the absolute path of each file to the list\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            repo_file_path = os.path.relpath(file_path, folder_path)\n",
        "            file_paths.append(repo_file_path)\n",
        "\n",
        "    return file_paths"
      ],
      "metadata": {
        "id": "B77L7e6z-Ez9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main retrieve function\n",
        "import os\n",
        "from llama_index.core import (\n",
        "    Document,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "def retrieve(\n",
        "    files,\n",
        "    prompt,\n",
        "    embedding_model,\n",
        "    save_dir,\n",
        "    top_k,\n",
        "    file_to_contents,\n",
        "):\n",
        "    # Check if index files exist in save_dir\n",
        "    if os.path.exists(save_dir) and os.path.isdir(save_dir) and os.listdir(save_dir):\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=save_dir)\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        print(\"Loading existing index from storage\")\n",
        "    else:\n",
        "        print(\"Creating new index\")\n",
        "        documents = []\n",
        "        for file in files:\n",
        "            file_content = file_to_contents[file]\n",
        "            meta_data = {\n",
        "                \"File Name\": file,\n",
        "            }\n",
        "            doc = Document(\n",
        "                text=file_content,\n",
        "                metadata=meta_data,\n",
        "                metadata_template=\"{key}: {value}\",\n",
        "                text_template=\"{metadata_str}\\n-----\\nCode:\\n{content}\",\n",
        "            )\n",
        "            documents.append(doc)\n",
        "\n",
        "        index = VectorStoreIndex.from_documents(documents, embed_model=embedding_model) #embedding_model)\n",
        "        index.storage_context.persist(persist_dir=save_dir)\n",
        "\n",
        "    retriever = VectorIndexRetriever(index=index, similarity_top_k=top_k)\n",
        "    retrieved_documents = retriever.retrieve(prompt)\n",
        "\n",
        "    file_names = []\n",
        "    file_contents = []\n",
        "    for node in retrieved_documents:\n",
        "        file = node.metadata[\"File Name\"]\n",
        "        if file not in file_names:\n",
        "            file_names.append(file)\n",
        "            file_contents.append(file_to_contents[file])\n",
        "\n",
        "    return file_names, file_contents"
      ],
      "metadata": {
        "id": "4swpqlLD-Fhu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "id": "4HjMA3SWaog2",
        "outputId": "8fb396b4-bed7-45d2-da91-388dfc4e1ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xformers\n",
            "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.4)\n",
            "Collecting torch==2.5.1 (from xformers)\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->xformers)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->xformers) (3.0.2)\n",
            "Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 triton-3.1.0 xformers-0.0.28.post3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "5c4da15587c843a2a15b1efc6c06960f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HF_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "RETRIEVAL_MODEL = HuggingFaceEmbedding(model_name=\"dunzhang/stella_en_400M_v5\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "_DLPWvu4LG-9",
        "outputId": "6e987f22-cbcc-456f-fd60-b82630f9793b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a11a1bb66840>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHF_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HUGGING_FACE_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuggingface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRETRIEVAL_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dunzhang/stella_en_400M_v5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent\n",
        "import json\n",
        "import os\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from random import shuffle\n",
        "import sys\n",
        "import pandas as pd\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "repo_to_top_folder = {\n",
        "    \"django/django\": \"django\",\n",
        "    \"sphinx-doc/sphinx\": \"sphinx\",\n",
        "    \"scikit-learn/scikit-learn\": \"scikit-learn\",\n",
        "    \"sympy/sympy\": \"sympy\",\n",
        "    \"pytest-dev/pytest\": \"pytest\",\n",
        "    \"matplotlib/matplotlib\": \"matplotlib\",\n",
        "    \"astropy/astropy\": \"astropy\",\n",
        "    \"pydata/xarray\": \"xarray\",\n",
        "    \"mwaskom/seaborn\": \"seaborn\",\n",
        "    \"psf/requests\": \"requests\",\n",
        "    \"pylint-dev/pylint\": \"pylint\",\n",
        "    \"pallets/flask\": \"flask\",\n",
        "}\n",
        "\n",
        "# Create locks for each repo to prevent concurrent access\n",
        "repo_locks = {repo: threading.Lock() for repo in repo_to_top_folder.keys()}\n",
        "output_lock = threading.Lock()\n",
        "\n",
        "splits = {\n",
        "    \"dev\": \"data/dev-00000-of-00001.parquet\",\n",
        "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
        "}\n",
        "swe_df = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench_Lite/\" + splits[\"test\"])\n",
        "\n",
        "RETRIEVAL_INSTRUCTION = (\n",
        "    \"\\n\\nFind code the code which need to be edited to solve the above issue\"\n",
        ")\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
        "RETRIEVAL_MODEL =HuggingFaceEmbedding(model_name=\"dunzhang/stella_en_400M_v5\", trust_remote_code=True)\n",
        "\n",
        "def retrieve_instance(args, row, output_path, existing_ids):\n",
        "    \"\"\"Process a single row from the DataFrame\"\"\"\n",
        "    # Skip if instance_id already exists\n",
        "    if row[\"instance_id\"] in existing_ids:\n",
        "        return None\n",
        "    print(row[\"instance_id\"])\n",
        "    original_prompt = row[\"problem_statement\"]\n",
        "    persist_dir = os.path.join(\n",
        "        args.base_path, \"fl\", \"embeddings_cornstack\", row[\"instance_id\"]\n",
        "    )\n",
        "\n",
        "    file_dir = os.path.join(args.testbed_dir, repo_to_top_folder[row[\"repo\"]])\n",
        "    full_prompt = original_prompt + RETRIEVAL_INSTRUCTION\n",
        "\n",
        "    with repo_locks[row[\"repo\"]]:\n",
        "        checkout_commit(file_dir, row[\"base_commit\"])\n",
        "        files = get_all_file_paths(file_dir, args.testbed_dir)\n",
        "\n",
        "        files = [\n",
        "            file_path\n",
        "            for file_path in files\n",
        "            if file_path.endswith(\".py\") and (not \"/test\" in file_path)\n",
        "        ]\n",
        "        file_to_contents = {}\n",
        "        for file in files:\n",
        "            with open(os.path.join(file_dir, file), \"r\") as f:\n",
        "                try:\n",
        "                    file_to_contents[file] = f.read()\n",
        "                except:\n",
        "                    print(\"Error in reading file\")\n",
        "\n",
        "    file_names, file_contents = retrieve(\n",
        "        list(file_to_contents.keys()),\n",
        "        full_prompt,\n",
        "        RETRIEVAL_MODEL,\n",
        "        persist_dir,\n",
        "        args.retrieve_num,\n",
        "        file_to_contents\n",
        "    )\n",
        "\n",
        "    result = {\n",
        "        \"instance_id\": row[\"instance_id\"],\n",
        "        \"problem_description\": original_prompt,\n",
        "        \"found_files\": file_names,\n",
        "        \"file_contents\": file_contents,\n",
        "    }\n",
        "\n",
        "    # Write the result to the output file with a lock to prevent concurrent writing\n",
        "    with output_lock:\n",
        "        with open(output_path, \"a\") as f:\n",
        "            json_line = json.dumps(result)\n",
        "            f.write(json_line + \"\\n\")\n",
        "            f.flush()\n",
        "\n",
        "    return result\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.base_path = \"/content/\"\n",
        "        self.testbed_dir = \"/content/testbed\"\n",
        "        self.retrieve_num = 50\n",
        "        self.output_file = \"retrieved_mbet.jsonl\"\n",
        "        self.max_workers = 1\n",
        "        self.embedding_dir = \"mbet_embeddings\"\n",
        "\n",
        "args = Args()\n",
        "\n",
        "output_path = os.path.join(args.base_path, args.output_file)\n",
        "# Get existing instance IDs\n",
        "existing_ids = get_existing_instance_ids(output_path)\n",
        "print(existing_ids)\n",
        "\n",
        "# Create file if it doesn't exist\n",
        "if not os.path.exists(output_path):\n",
        "    with open(output_path, \"w\") as f:\n",
        "        pass\n",
        "\n",
        "if args.max_workers == 1:\n",
        "    for _, row in swe_df.iterrows():\n",
        "        retrieve_instance(args, row, output_path, existing_ids)\n",
        "else:\n",
        "    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:\n",
        "        rows_list = list(swe_df.iterrows())\n",
        "        shuffle(rows_list)\n",
        "\n",
        "        future_to_row = {\n",
        "            executor.submit(retrieve_instance, args, row, output_path, existing_ids): row\n",
        "            for _, row in rows_list\n",
        "        }\n",
        "\n",
        "        for future in concurrent.futures.as_completed(future_to_row):\n",
        "            row = future_to_row[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row: {e}\")"
      ],
      "metadata": {
        "id": "Hi3XxAfHyuD7",
        "outputId": "5382e680-6c98-44fd-87ca-b15f7ce1c279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 3.0.1, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
            "\n",
            "\n",
            "\n",
            "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
            "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Dense.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path exists\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{'django__django-15814', 'django__django-16046', 'django__django-14608', 'pydata__xarray-4248', 'pytest-dev__pytest-5221', 'django__django-12113', 'scikit-learn__scikit-learn-10297', 'scikit-learn__scikit-learn-13584', 'sympy__sympy-13647', 'sympy__sympy-12171', 'django__django-13658', 'django__django-14787', 'django__django-16255', 'astropy__astropy-12907', 'pytest-dev__pytest-7168', 'matplotlib__matplotlib-22835', 'sympy__sympy-15308', 'sympy__sympy-13895', 'sympy__sympy-17630', 'matplotlib__matplotlib-25332', 'sympy__sympy-21055', 'scikit-learn__scikit-learn-14092', 'sympy__sympy-18189', 'pytest-dev__pytest-8365', 'django__django-14730', 'django__django-13321', 'sphinx-doc__sphinx-8474', 'sympy__sympy-22005', 'sympy__sympy-12481', 'pytest-dev__pytest-5413', 'django__django-12908', 'matplotlib__matplotlib-23987', 'sympy__sympy-13437', 'django__django-12856', 'sympy__sympy-19007', 'sympy__sympy-13480', 'scikit-learn__scikit-learn-13496', 'django__django-11630', 'django__django-13757', 'sympy__sympy-20049', 'sympy__sympy-15678', 'sphinx-doc__sphinx-8801', 'sympy__sympy-18835', 'django__django-14411', 'sphinx-doc__sphinx-8273', 'django__django-12308', 'django__django-13710', 'django__django-14238', 'sympy__sympy-20212', 'pallets__flask-4045', 'django__django-11620', 'django__django-10924', 'django__django-13933', 'django__django-12708', 'matplotlib__matplotlib-23913', 'django__django-11133', 'django__django-13768', 'sympy__sympy-13915', 'django__django-15388', 'sympy__sympy-14396', 'django__django-12470', 'django__django-11848', 'sympy__sympy-22840', 'pytest-dev__pytest-8906', 'scikit-learn__scikit-learn-25638', 'sphinx-doc__sphinx-8435', 'pydata__xarray-4094', 'django__django-15252', 'matplotlib__matplotlib-23964', 'pallets__flask-4992', 'astropy__astropy-14995', 'sympy__sympy-18087', 'sympy__sympy-20442', 'sympy__sympy-13031', 'django__django-11049', 'scikit-learn__scikit-learn-10508', 'django__django-14580', 'django__django-15400', 'django__django-13220', 'sympy__sympy-15011', 'scikit-learn__scikit-learn-14087', 'scikit-learn__scikit-learn-25570', 'sympy__sympy-21614', 'sympy__sympy-16281', 'django__django-17087', 'matplotlib__matplotlib-25079', 'sympy__sympy-20639', 'django__django-14915', 'scikit-learn__scikit-learn-15535', 'psf__requests-2674', 'sympy__sympy-24152', 'django__django-13265', 'django__django-14999', 'django__django-12700', 'sympy__sympy-18621', 'django__django-11905', 'sympy__sympy-16503', 'mwaskom__seaborn-3407', 'django__django-16408', 'pytest-dev__pytest-6116', 'django__django-14855', 'sphinx-doc__sphinx-10325', 'sphinx-doc__sphinx-11445', 'scikit-learn__scikit-learn-12471', 'django__django-11039', 'pytest-dev__pytest-7490', 'matplotlib__matplotlib-24149', 'sphinx-doc__sphinx-8282', 'sympy__sympy-15346', 'matplotlib__matplotlib-25433', 'sphinx-doc__sphinx-8627', 'scikit-learn__scikit-learn-14894', 'sphinx-doc__sphinx-8595', 'django__django-14382', 'pydata__xarray-4493', 'astropy__astropy-6938', 'django__django-12286', 'matplotlib__matplotlib-23314', 'mwaskom__seaborn-2848', 'matplotlib__matplotlib-23299', 'sphinx-doc__sphinx-8721', 'pytest-dev__pytest-7220', 'matplotlib__matplotlib-24334', 'pytest-dev__pytest-5227', 'pytest-dev__pytest-9359', 'django__django-16910', 'matplotlib__matplotlib-22711', 'sympy__sympy-14774', 'matplotlib__matplotlib-26011', 'sympy__sympy-12236', 'scikit-learn__scikit-learn-13497', 'django__django-11099', 'django__django-13033', 'sympy__sympy-24909', 'sympy__sympy-19487', 'matplotlib__matplotlib-23562', 'django__django-16041', 'scikit-learn__scikit-learn-11040', 'django__django-14016', 'pydata__xarray-3364', 'pytest-dev__pytest-11143', 'sympy__sympy-13043', 'sympy__sympy-18199', 'psf__requests-1963', 'matplotlib__matplotlib-24970', 'scikit-learn__scikit-learn-11281', 'django__django-16527', 'django__django-15202', 'django__django-12589', 'django__django-15789', 'django__django-11019', 'django__django-16873', 'django__django-16595', 'django__django-13315', 'mwaskom__seaborn-3010', 'django__django-16229', 'sympy__sympy-21171', 'sympy__sympy-21627', 'django__django-15213', 'sympy__sympy-17022', 'django__django-15738', 'scikit-learn__scikit-learn-13142', 'pytest-dev__pytest-7432', 'sympy__sympy-16988', 'mwaskom__seaborn-3190', 'sympy__sympy-21379', 'django__django-16379', 'sympy__sympy-11870', 'django__django-10914', 'sympy__sympy-11897', 'django__django-15347', 'django__django-12915', 'sympy__sympy-17139', 'django__django-11422', 'scikit-learn__scikit-learn-13241', 'sympy__sympy-21612', 'django__django-14534', 'astropy__astropy-14365', 'django__django-11815', 'sympy__sympy-20154', 'sphinx-doc__sphinx-10451', 'psf__requests-2317', 'django__django-13401', 'django__django-15061', 'sympy__sympy-18057', 'sympy__sympy-14024', 'django__django-14155', 'psf__requests-863', 'sympy__sympy-13177', 'django__django-11001', 'scikit-learn__scikit-learn-25747', 'django__django-11283', 'sympy__sympy-23262', 'django__django-15320', 'sympy__sympy-18532', 'sympy__sympy-16792', 'django__django-13964', 'django__django-13448', 'scikit-learn__scikit-learn-13439', 'django__django-15781', 'django__django-15819', 'sympy__sympy-24213', 'sympy__sympy-23117', 'matplotlib__matplotlib-25498', 'psf__requests-2148', 'matplotlib__matplotlib-18869', 'matplotlib__matplotlib-23563', 'pytest-dev__pytest-11148', 'psf__requests-3362', 'django__django-13660', 'django__django-11564', 'astropy__astropy-14182', 'scikit-learn__scikit-learn-15512', 'django__django-14667', 'matplotlib__matplotlib-26020', 'django__django-16816', 'sympy__sympy-12419', 'django__django-13230', 'sphinx-doc__sphinx-8713', 'django__django-16400', 'django__django-13447', 'sympy__sympy-23191', 'django__django-17051', 'astropy__astropy-7746', 'django__django-12983', 'sympy__sympy-16106', 'django__django-14752', 'django__django-15695', 'sympy__sympy-12454', 'sympy__sympy-13971', 'sympy__sympy-13773', 'django__django-12284', 'django__django-13925', 'sympy__sympy-14308', 'sympy__sympy-19254', 'sympy__sympy-24066', 'sphinx-doc__sphinx-7738', 'sympy__sympy-18698', 'sympy__sympy-14817', 'django__django-11179', 'django__django-14672', 'pytest-dev__pytest-5495', 'scikit-learn__scikit-learn-10949', 'django__django-15851', 'scikit-learn__scikit-learn-25500', 'django__django-11999', 'sympy__sympy-21847', 'django__django-12125', 'pytest-dev__pytest-5103', 'scikit-learn__scikit-learn-14983', 'django__django-11583', 'django__django-13028', 'sympy__sympy-20322', 'sphinx-doc__sphinx-7975', 'pytest-dev__pytest-5692', 'django__django-15996', 'sphinx-doc__sphinx-8506', 'sympy__sympy-17655', 'django__django-13551', 'sympy__sympy-20590', 'sympy__sympy-13471', 'matplotlib__matplotlib-25311', 'django__django-16139', 'sphinx-doc__sphinx-7686', 'sympy__sympy-15345', 'sympy__sympy-22714', 'sympy__sympy-14317', 'matplotlib__matplotlib-25442', 'django__django-15902', 'django__django-13158', 'django__django-11910', 'pallets__flask-5063', 'django__django-14017', 'matplotlib__matplotlib-24265', 'sympy__sympy-11400', 'pytest-dev__pytest-7373', 'django__django-12453', 'pydata__xarray-5131', 'django__django-12497', 'django__django-11797', 'django__django-11742', 'sympy__sympy-24102', 'django__django-12184', 'django__django-11964', 'scikit-learn__scikit-learn-13779', 'django__django-15498', 'django__django-13590', 'django__django-16820', 'django__django-12747', 'django__django-14997', 'django__django-15790', 'sympy__sympy-15609', 'matplotlib__matplotlib-23476', 'sympy__sympy-13146'}\n",
            "pylint-dev__pylint-5859\n",
            "Cleaning untracked files in repository at /content/testbed/pylint...\n",
            "Discarding changes in tracked files...\n",
            "Checking out commit 182cc539b8154c0710fcea7e522267e42eba8899...\n",
            "Commit checked out successfully.\n",
            "Error in reading file\n",
            "Creating new index\n",
            "pylint-dev__pylint-6506\n",
            "Cleaning untracked files in repository at /content/testbed/pylint...\n",
            "Discarding changes in tracked files...\n",
            "Checking out commit 0a4204fd7555cfedd43f43017c94d24ef48244a5...\n",
            "Commit checked out successfully.\n",
            "Error in reading file\n",
            "Creating new index\n",
            "pylint-dev__pylint-7080\n",
            "Cleaning untracked files in repository at /content/testbed/pylint...\n",
            "Discarding changes in tracked files...\n",
            "Checking out commit 3c5eca2ded3dd2b59ebaf23eb289453b5d2930f0...\n",
            "Commit checked out successfully.\n",
            "Error in reading file\n",
            "Creating new index\n",
            "pylint-dev__pylint-7114\n",
            "Cleaning untracked files in repository at /content/testbed/pylint...\n",
            "Discarding changes in tracked files...\n",
            "Checking out commit 397c1703e8ae6349d33f7b99f45b2ccaf581e666...\n",
            "Commit checked out successfully.\n",
            "Error in reading file\n",
            "Creating new index\n",
            "pylint-dev__pylint-7228\n",
            "Cleaning untracked files in repository at /content/testbed/pylint...\n",
            "Discarding changes in tracked files...\n",
            "Checking out commit d597f252915ddcaaa15ccdfcb35670152cb83587...\n",
            "Commit checked out successfully.\n",
            "Error in reading file\n",
            "Creating new index\n",
            "pylint-dev__pylint-7993\n",
            "Cleaning untracked files in repository at /content/testbed/pylint...\n",
            "Discarding changes in tracked files...\n",
            "Checking out commit e90702074e68e20dc8e5df5013ee3ecf22139c3e...\n",
            "Commit checked out successfully.\n",
            "Error in reading file\n",
            "Creating new index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "output_path = \"/content/retrieved_mbet.jsonl\"\n",
        "def get_existing_instance_ids(output_path):\n",
        "    \"\"\"Read existing instance IDs from the output file\"\"\"\n",
        "    existing_ids = set()\n",
        "    if os.path.exists(output_path):\n",
        "        print(\"path exists\")\n",
        "        with open(output_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line.strip())\n",
        "                print()\n",
        "                existing_ids.add(data['instance_id'])\n",
        "    return existing_ids\n",
        "existing_ids = get_existing_instance_ids(output_path)\n",
        "print(existing_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsC_YB36NeOq",
        "outputId": "62a6c71c-6bda-465d-b4f3-28b0b728087f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path exists\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "{'django__django-15814', 'django__django-16046', 'django__django-14608', 'pydata__xarray-4248', 'pytest-dev__pytest-5221', 'django__django-12113', 'scikit-learn__scikit-learn-10297', 'scikit-learn__scikit-learn-13584', 'sympy__sympy-13647', 'sympy__sympy-12171', 'django__django-13658', 'django__django-14787', 'django__django-16255', 'astropy__astropy-12907', 'pytest-dev__pytest-7168', 'matplotlib__matplotlib-22835', 'sympy__sympy-15308', 'sympy__sympy-13895', 'sympy__sympy-17630', 'matplotlib__matplotlib-25332', 'sympy__sympy-21055', 'scikit-learn__scikit-learn-14092', 'sympy__sympy-18189', 'pytest-dev__pytest-8365', 'django__django-14730', 'django__django-13321', 'sphinx-doc__sphinx-8474', 'sympy__sympy-22005', 'sympy__sympy-12481', 'pytest-dev__pytest-5413', 'django__django-12908', 'matplotlib__matplotlib-23987', 'sympy__sympy-13437', 'django__django-12856', 'sympy__sympy-19007', 'sympy__sympy-13480', 'scikit-learn__scikit-learn-13496', 'django__django-11630', 'django__django-13757', 'sympy__sympy-20049', 'sympy__sympy-15678', 'sphinx-doc__sphinx-8801', 'sympy__sympy-18835', 'django__django-14411', 'sphinx-doc__sphinx-8273', 'django__django-12308', 'django__django-13710', 'django__django-14238', 'sympy__sympy-20212', 'pallets__flask-4045', 'django__django-11620', 'django__django-10924', 'django__django-13933', 'django__django-12708', 'matplotlib__matplotlib-23913', 'django__django-11133', 'django__django-13768', 'sympy__sympy-13915', 'django__django-15388', 'sympy__sympy-14396', 'django__django-12470', 'django__django-11848', 'sympy__sympy-22840', 'pytest-dev__pytest-8906', 'scikit-learn__scikit-learn-25638', 'sphinx-doc__sphinx-8435', 'pydata__xarray-4094', 'django__django-15252', 'matplotlib__matplotlib-23964', 'pallets__flask-4992', 'astropy__astropy-14995', 'sympy__sympy-18087', 'sympy__sympy-20442', 'sympy__sympy-13031', 'django__django-11049', 'scikit-learn__scikit-learn-10508', 'django__django-14580', 'django__django-15400', 'django__django-13220', 'sympy__sympy-15011', 'scikit-learn__scikit-learn-14087', 'scikit-learn__scikit-learn-25570', 'sympy__sympy-21614', 'sympy__sympy-16281', 'django__django-17087', 'matplotlib__matplotlib-25079', 'sympy__sympy-20639', 'django__django-14915', 'scikit-learn__scikit-learn-15535', 'psf__requests-2674', 'sympy__sympy-24152', 'django__django-13265', 'django__django-14999', 'django__django-12700', 'sympy__sympy-18621', 'django__django-11905', 'sympy__sympy-16503', 'mwaskom__seaborn-3407', 'django__django-16408', 'pytest-dev__pytest-6116', 'django__django-14855', 'sphinx-doc__sphinx-10325', 'sphinx-doc__sphinx-11445', 'scikit-learn__scikit-learn-12471', 'django__django-11039', 'pytest-dev__pytest-7490', 'matplotlib__matplotlib-24149', 'sphinx-doc__sphinx-8282', 'sympy__sympy-15346', 'matplotlib__matplotlib-25433', 'sphinx-doc__sphinx-8627', 'scikit-learn__scikit-learn-14894', 'sphinx-doc__sphinx-8595', 'django__django-14382', 'pydata__xarray-4493', 'astropy__astropy-6938', 'django__django-12286', 'matplotlib__matplotlib-23314', 'mwaskom__seaborn-2848', 'matplotlib__matplotlib-23299', 'sphinx-doc__sphinx-8721', 'pytest-dev__pytest-7220', 'matplotlib__matplotlib-24334', 'pytest-dev__pytest-5227', 'pytest-dev__pytest-9359', 'django__django-16910', 'matplotlib__matplotlib-22711', 'sympy__sympy-14774', 'matplotlib__matplotlib-26011', 'sympy__sympy-12236', 'scikit-learn__scikit-learn-13497', 'django__django-11099', 'django__django-13033', 'sympy__sympy-24909', 'sympy__sympy-19487', 'matplotlib__matplotlib-23562', 'django__django-16041', 'scikit-learn__scikit-learn-11040', 'django__django-14016', 'pydata__xarray-3364', 'pytest-dev__pytest-11143', 'sympy__sympy-13043', 'sympy__sympy-18199', 'psf__requests-1963', 'matplotlib__matplotlib-24970', 'scikit-learn__scikit-learn-11281', 'django__django-16527', 'django__django-15202', 'django__django-12589', 'django__django-15789', 'django__django-11019', 'django__django-16873', 'django__django-16595', 'django__django-13315', 'mwaskom__seaborn-3010', 'django__django-16229', 'sympy__sympy-21171', 'sympy__sympy-21627', 'django__django-15213', 'sympy__sympy-17022', 'django__django-15738', 'scikit-learn__scikit-learn-13142', 'pytest-dev__pytest-7432', 'sympy__sympy-16988', 'mwaskom__seaborn-3190', 'sympy__sympy-21379', 'django__django-16379', 'sympy__sympy-11870', 'django__django-10914', 'sympy__sympy-11897', 'django__django-15347', 'django__django-12915', 'sympy__sympy-17139', 'django__django-11422', 'scikit-learn__scikit-learn-13241', 'sympy__sympy-21612', 'django__django-14534', 'astropy__astropy-14365', 'django__django-11815', 'sympy__sympy-20154', 'sphinx-doc__sphinx-10451', 'psf__requests-2317', 'django__django-13401', 'django__django-15061', 'sympy__sympy-18057', 'sympy__sympy-14024', 'django__django-14155', 'psf__requests-863', 'sympy__sympy-13177', 'django__django-11001', 'scikit-learn__scikit-learn-25747', 'django__django-11283', 'sympy__sympy-23262', 'django__django-15320', 'sympy__sympy-18532', 'sympy__sympy-16792', 'django__django-13964', 'django__django-13448', 'scikit-learn__scikit-learn-13439', 'django__django-15781', 'django__django-15819', 'sympy__sympy-24213', 'sympy__sympy-23117', 'matplotlib__matplotlib-25498', 'psf__requests-2148', 'matplotlib__matplotlib-18869', 'matplotlib__matplotlib-23563', 'pytest-dev__pytest-11148', 'psf__requests-3362', 'django__django-13660', 'django__django-11564', 'astropy__astropy-14182', 'scikit-learn__scikit-learn-15512', 'django__django-14667', 'matplotlib__matplotlib-26020', 'django__django-16816', 'sympy__sympy-12419', 'django__django-13230', 'sphinx-doc__sphinx-8713', 'django__django-16400', 'django__django-13447', 'sympy__sympy-23191', 'django__django-17051', 'astropy__astropy-7746', 'django__django-12983', 'sympy__sympy-16106', 'django__django-14752', 'django__django-15695', 'sympy__sympy-12454', 'sympy__sympy-13971', 'sympy__sympy-13773', 'django__django-12284', 'django__django-13925', 'sympy__sympy-14308', 'sympy__sympy-19254', 'sympy__sympy-24066', 'sphinx-doc__sphinx-7738', 'sympy__sympy-18698', 'sympy__sympy-14817', 'django__django-11179', 'django__django-14672', 'pytest-dev__pytest-5495', 'scikit-learn__scikit-learn-10949', 'django__django-15851', 'scikit-learn__scikit-learn-25500', 'django__django-11999', 'sympy__sympy-21847', 'django__django-12125', 'pytest-dev__pytest-5103', 'scikit-learn__scikit-learn-14983', 'django__django-11583', 'django__django-13028', 'sympy__sympy-20322', 'sphinx-doc__sphinx-7975', 'pytest-dev__pytest-5692', 'django__django-15996', 'sphinx-doc__sphinx-8506', 'sympy__sympy-17655', 'django__django-13551', 'sympy__sympy-20590', 'sympy__sympy-13471', 'matplotlib__matplotlib-25311', 'django__django-16139', 'sphinx-doc__sphinx-7686', 'sympy__sympy-15345', 'sympy__sympy-22714', 'sympy__sympy-14317', 'matplotlib__matplotlib-25442', 'django__django-15902', 'django__django-13158', 'django__django-11910', 'pallets__flask-5063', 'django__django-14017', 'matplotlib__matplotlib-24265', 'sympy__sympy-11400', 'pytest-dev__pytest-7373', 'django__django-12453', 'pydata__xarray-5131', 'django__django-12497', 'django__django-11797', 'django__django-11742', 'sympy__sympy-24102', 'django__django-12184', 'django__django-11964', 'scikit-learn__scikit-learn-13779', 'django__django-15498', 'django__django-13590', 'django__django-16820', 'django__django-12747', 'django__django-14997', 'django__django-15790', 'sympy__sympy-15609', 'matplotlib__matplotlib-23476', 'sympy__sympy-13146'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvaxLSRbNfvw",
        "outputId": "b92ef855-7498-46c3-9b55-f88fe310e019"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/retrieved_mbet.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcqFhH0Tw8sn",
        "outputId": "b1da77b5-e0a3-4cfc-cfa3-22075fa499b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n",
            "[-0.003275713650509715, -0.011690733022987843, 0.041559211909770966, -0.038148071616888046, 0.024183085188269615]\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38S5oaRSw8sn"
      },
      "source": [
        "## InstructorEmbedding\n",
        "\n",
        "Instructor Embeddings are a class of embeddings specifically trained to augment their embeddings according to an instruction. By default, queries are given `query_instruction=\"Represent the question for retrieving supporting documents: \"` and text is given `text_instruction=\"Represent the document for retrieval: \"`.\n",
        "\n",
        "They rely on the `Instructor` and `SentenceTransformers` (version 2.2.2) pip package, which you can install with `pip install InstructorEmbedding` and `pip install -U sentence-transformers==2.2.2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwzKV98Yw8so",
        "outputId": "a8a35d76-5264-4218-a2c6-fb909e3c9f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-29cfa5294e9d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstructor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstructorEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstructorEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hkunlp/instructor-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/instructor/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, query_instruction, text_instruction, embed_batch_size, cache_folder, device, callback_manager)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcache_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINSTRUCTOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_sentence_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 modules = self._load_sbert_model(\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.instructor import InstructorEmbedding\n",
        "\n",
        "embed_model = InstructorEmbedding(model_name=\"hkunlp/instructor-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwR783JLw8so",
        "outputId": "6ac613fc-23ec-4f84-a2a8-ab71952f37d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n",
            "[ 0.02155361 -0.06098218  0.01796207  0.05490903  0.01526906]\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVf6QJ5lw8so"
      },
      "source": [
        "## OptimumEmbedding\n",
        "\n",
        "Optimum in a HuggingFace library for exporting and running HuggingFace models in the ONNX format.\n",
        "\n",
        "You can install the dependencies with `pip install transformers optimum[exporters]`.\n",
        "\n",
        "First, we need to create the ONNX model. ONNX models provide improved inference speeds, and can be used across platforms (i.e. in TransformersJS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6uI_qZSw8so",
        "outputId": "d48c489e-52c5-460a-b275-b0441ce822b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "Framework not specified. Using pt to export to ONNX.\n",
            "Using the export variant default. Available variants are:\n",
            "\t- default: The default ONNX variant.\n",
            "Using framework PyTorch: 2.0.1+cu117\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "Saved optimum model to ./bge_onnx. Use it with `embed_model = OptimumEmbedding(folder_name='./bge_onnx')`.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
        "\n",
        "OptimumEmbedding.create_and_save_optimum_model(\n",
        "    \"BAAI/bge-small-en-v1.5\", \"./bge_onnx\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJSmvUOow8so"
      },
      "outputs": [],
      "source": [
        "embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRbdNr1pw8so",
        "outputId": "b8a32cd1-6e13-4d51-8a0c-36918876d9db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "384\n",
            "[-0.10364960134029388, -0.20998482406139374, -0.01883639395236969, -0.5241696834564209, 0.0335749015212059]\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBA_xhZfw8so"
      },
      "source": [
        "## Benchmarking\n",
        "\n",
        "Let's try comparing using a classic large document -- the IPCC climate report, chapter 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnZJ4RPGw8sp",
        "outputId": "02766485-43e0-4ea5-904b-c3b91849cf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 20.7M  100 20.7M    0     0  16.5M      0  0:00:01  0:00:01 --:--:-- 16.5M\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "434uPetLw8sp"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UCCpNbw8sp"
      },
      "source": [
        "### Base HuggingFace Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B6l9hj0w8sp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# needed to synthesize responses later\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW290ymbw8sp"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n",
        "\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FrC24f9w8sq",
        "outputId": "8c2cc598-5fa3-43dc-dae1-5c8644006e1b",
        "colab": {
          "referenced_widgets": [
            "3bfb6c2115c447e5af19bb18b5a07ebe",
            "55c188c3d18049df933cd87bd5a49ed1"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bfb6c2115c447e5af19bb18b5a07ebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing documents into nodes:   0%|          | 0/172 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55c188c3d18049df933cd87bd5a49ed1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/428 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1min 27s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -r 1 -n 1\n",
        "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKiFj5ivw8sq"
      },
      "source": [
        "### Optimum Embeddings\n",
        "\n",
        "We can use the onnx embeddings we created earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA6j0OFJw8sq"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
        "\n",
        "embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")\n",
        "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n",
        "\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEIg1TmGw8sq",
        "outputId": "69ef4e3e-2690-4904-e2d0-518f3c876360",
        "colab": {
          "referenced_widgets": [
            "cf49fd773c6f486f9fee323db590bcd5",
            "7f1f3f8a5d3140bd9982a158a6ef9b9f"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf49fd773c6f486f9fee323db590bcd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing documents into nodes:   0%|          | 0/172 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f1f3f8a5d3140bd9982a158a6ef9b9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/428 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1min 9s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -r 1 -n 1\n",
        "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama-index",
      "language": "python",
      "name": "llama-index"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}